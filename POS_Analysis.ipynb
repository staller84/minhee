{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staller84/minhee/blob/master/feature_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShduHbjzkEMk",
        "colab_type": "text"
      },
      "source": [
        "# 형태소 분석 테스트를 위한 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga_RUylcj_nt",
        "colab_type": "code",
        "outputId": "f1c53bd1-369d-4f61-dbfd-ee387f7f1682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git\n",
        "!pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 18.02 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 120kB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 45.3MB/s \n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.7.0 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq95u8u7kw3z",
        "colab_type": "code",
        "outputId": "35259c56-663f-4a1d-ff4b-b66ccfeea133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!cat nsmc/ratings_train.txt | head -n 10"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id\tdocument\tlabel\n",
            "9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
            "3819312\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t1\n",
            "10265843\t너무재밓었다그래서보는것을추천한다\t0\n",
            "9045019\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t0\n",
            "6483659\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\t1\n",
            "5403919\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\t0\n",
            "7797314\t원작의 긴장감을 제대로 살려내지못했다.\t0\n",
            "9443947\t별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\t0\n",
            "7156791\t액션이 없는데도 재미 있는 몇안되는 영화\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhYAcjm8kN0A",
        "colab_type": "text"
      },
      "source": [
        "###### 패키지 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt0mTuEpkRnF",
        "colab_type": "code",
        "outputId": "76857eaa-2bec-42b8-f5fa-064a73f41350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# code\n",
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint\n",
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOc3_FykXyt",
        "colab_type": "text"
      },
      "source": [
        "###### 파일은 \\t을 기준으로 구성되어 있음.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsOj9ZXgkjyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        # txt 파일의 헤더(id document label)는 제외하기\n",
        "        data = data[1:]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMuos5Yg3SHF",
        "colab_type": "text"
      },
      "source": [
        "###### 토큰화 방법 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz_LOwFY3ZUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(doc, morph, case_check):\n",
        "    if case_check == \"Hannanum\":\n",
        "        return ['/'.join(t) for t in morph.pos(doc)]\n",
        "    elif case_check == \"Kkma\":\n",
        "        return ['/'.join(t) for t in morph.pos(doc)]  \n",
        "    elif case_check == \"Komoran\":\n",
        "        return ['/'.join(t) for t in morph.pos(doc)]    \n",
        "    elif case_check == \"Okt\":\n",
        "        return ['/'.join(t) for t in morph.pos(doc)]\n",
        "    elif case_check == \"Syl\":\n",
        "        return [t for t in doc]\n",
        "    else:\n",
        "        data = doc.split(\" \")\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsV2a_mK3aAc",
        "colab_type": "text"
      },
      "source": [
        "###### 토큰화 시켜서 데이터 재생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4SxGfBJ4WZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_data(morph, case_check):\n",
        "    train_data = read_data('./nsmc/ratings_train.txt')\n",
        "    test_data = read_data('./nsmc/ratings_test.txt')\n",
        "    train_data = train_data[:10000]\n",
        "    test_data = test_data[:1000]\n",
        "    print(\"학습데이터 형태 및 수\")\n",
        "    print(train_data[:1])\n",
        "    print(len(train_data))\n",
        "    print()\n",
        "    print(\"테스트데이터 형태 및 수\")\n",
        "    print(len(test_data))\n",
        "    print(test_data[:1])\n",
        "    print()\n",
        "    \n",
        "    print(\"gen new data\")\n",
        "    train_docs = [(tokenize(row[1], morph, case_check), row[2]) for row in train_data]\n",
        "    test_docs = [(tokenize(row[1], morph, case_check), row[2]) for row in test_data]\n",
        "    \n",
        "    return train_docs, test_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVfPNwzY4Xut",
        "colab_type": "text"
      },
      "source": [
        "###### 토큰 갯수 세기 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbX02x935GIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_token(train_docs):\n",
        "    tokens = [t for d in train_docs for t in d[0]]\n",
        "    print(\"토큰수를 세기위한 샘플 토큰\")\n",
        "    print(tokens[0])\n",
        "    print()\n",
        "    text = nltk.Text(tokens, name='NMSC')\n",
        "    \n",
        "    # 전체 토큰의 개수\n",
        "    print(\"전체 토큰의 개수\")\n",
        "    print(len(text.tokens))\n",
        "    print()\n",
        "    # 중복을 제외한 토큰의 개수\n",
        "    print(\"중복을 제외한 토큰의 개수\")\n",
        "    print(len(set(text.tokens)))\n",
        "    print()\n",
        "    # 출현 빈도가 높은 상위 토큰 10개\n",
        "    print(\"출현 빈도가 높은 상위 토큰 10개\")\n",
        "    print(text.vocab().most_common(10))\n",
        "    print()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eytt2J0o5Gp1",
        "colab_type": "text"
      },
      "source": [
        "###### 모델선언\n",
        "###### CountVectorization을 사용하여 데이터를 벡터화\n",
        "###### 문서집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 Bag Of Words 인코딩 방식으로 벡터를 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qqB8Eij5zJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def term_frequency(doc, selected_words):\n",
        "    return [doc.count(word) for word in selected_words]\n",
        "\n",
        "\n",
        "def set_input(text, train_docs, test_docs):\n",
        "    selected_words = [f[0] for f in text.vocab().most_common(1500)]\n",
        "    train_x = [term_frequency(d, selected_words) for d, _ in train_docs]\n",
        "    test_x = [term_frequency(d, selected_words) for d, _ in test_docs]\n",
        "    train_y = [c for _, c in train_docs]\n",
        "    test_y = [c for _, c in test_docs]\n",
        "    \n",
        "    # data 형 변환\n",
        "    x_train = np.asarray(train_x).astype('float32')\n",
        "    x_test = np.asarray(test_x).astype('float32')\n",
        "    y_train = np.asarray(train_y).astype('float32')\n",
        "    y_test = np.asarray(test_y).astype('float32')\n",
        "    return selected_words, x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def set_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu', input_shape=(1500,)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiRHX8aO5k3N",
        "colab_type": "text"
      },
      "source": [
        "###### 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QdJyF3I5u3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, x_train, y_train):\n",
        "    print(\"\\n training part\")\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjDUWd0t5rWo",
        "colab_type": "text"
      },
      "source": [
        "###### 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc4fnAMU5mjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, x_test, y_test):\n",
        "    print(\"\\n evaluation part\")\n",
        "    results = model.evaluate(x_test, y_test)\n",
        "    print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SX2NOMq52Cs",
        "colab_type": "text"
      },
      "source": [
        "###### 메인함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaGOoK7dG5yE",
        "colab_type": "code",
        "outputId": "9a5fdef0-7921-4c53-edae-9fb589837bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # 1 한나눔 형태소 = 72.1%\n",
        "#     case_check = \"Hannanum\"\n",
        "#     morph = Hannanum()\n",
        "\n",
        "    # 2 꼬꼬마 형태소 = 80.1%\n",
        "#     case_check=\"Kkma\"\n",
        "#     morph = Kkma()\n",
        "    \n",
        "    # 3 코모란 형태소 = 76.9%\n",
        "#     case_check=\"Komoran\"\n",
        "#     morph = Komoran()\n",
        "\n",
        "    # 4 Okt 형태소 = 77.4%\n",
        "    # case_check=\"Okt\"\n",
        "    # morph = Okt()\n",
        "\n",
        "    # 5 단순 어절 = 66.7%\n",
        "    case_check = None\n",
        "    morph = None\n",
        "    \n",
        "    # 6 단순 음절 = 76.1%\n",
        "#     case_check = \"Syl\"\n",
        "#     morph = None\n",
        "    \n",
        "\n",
        "    train_docs, test_docs = gen_data(morph, case_check)\n",
        "    # 학습데이터 샘플\n",
        "    print(train_docs[0])\n",
        "    \n",
        "    text = count_token(train_docs)\n",
        "    selected_words, x_train, x_test, y_train, y_test = set_input(text, train_docs, test_docs)\n",
        "    \n",
        "    model = set_model()\n",
        "\n",
        "    model = train_model(model, x_train, y_train)\n",
        "    eval_model(model, x_test, y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습데이터 형태 및 수\n",
            "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0']]\n",
            "10000\n",
            "\n",
            "테스트데이터 형태 및 수\n",
            "1000\n",
            "[['6270596', '굳 ㅋ', '1']]\n",
            "\n",
            "gen new data\n",
            "(['아', '더빙..', '진짜', '짜증나네요', '목소리'], '0')\n",
            "토큰수를 세기위한 샘플 토큰\n",
            "아\n",
            "\n",
            "전체 토큰의 개수\n",
            "77210\n",
            "\n",
            "중복을 제외한 토큰의 개수\n",
            "40299\n",
            "\n",
            "출현 빈도가 높은 상위 토큰 10개\n",
            "[('영화', 737), ('정말', 534), ('너무', 508), ('진짜', 391), ('이', 330), ('영화.', 262), ('그냥', 235), ('이런', 233), ('왜', 223), ('더', 209)]\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            " training part\n",
            "Train on 10000 samples\n",
            "Epoch 1/5\n",
            "10000/10000 [==============================] - 1s 125us/sample - loss: 0.6157 - acc: 0.6488\n",
            "Epoch 2/5\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: 0.4775 - acc: 0.7411\n",
            "Epoch 3/5\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: 0.4187 - acc: 0.7729\n",
            "Epoch 4/5\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: 0.3713 - acc: 0.7988\n",
            "Epoch 5/5\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: 0.3268 - acc: 0.8232\n",
            "\n",
            " evaluation part\n",
            "1000/1000 [==============================] - 0s 55us/sample - loss: 0.7407 - acc: 0.6760\n",
            "[0.7406788538694382, 0.676]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2tsA0VWnHEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
