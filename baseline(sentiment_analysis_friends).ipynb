{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staller84/minhee/blob/master/baseline(sentiment_analysis_friends).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPYyPLERU-H",
        "colab_type": "text"
      },
      "source": [
        "# **Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT4PIM2xRgBp",
        "colab_type": "text"
      },
      "source": [
        "- Edit > Notebook settings > Hardward accelerators > GPU > SAVE\n",
        "- Download the Friends dataset in EmotionLines website:\n",
        "http://doraemon.iis.sinica.edu.tw/emotionlines/download.html\n",
        "- Download the unlabeled json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep3aCC6fqx1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idM2mzIOxygV",
        "colab_type": "text"
      },
      "source": [
        "# **Tutorials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NWok7mf5xZk",
        "colab_type": "text"
      },
      "source": [
        "##### **Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZewOX-D9BLJ",
        "colab_type": "code",
        "outputId": "c730aa26-7743-4fd7-811e-b0767dfb5f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers --quiet # package installer for python tranformar packe 2가지가 반복적인 구조를 가진다."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-89ad2474f81c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers --quiet # package installer for python tranformar packe 2가지가 반복적인 구조를 가진다.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: CustomError: Timed out waiting for output iframe load."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6idZebToz_Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKYIuzDw6mCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = 'bert-base-uncased'  # pretrained 된걸 release해야 burt가 모두 구조적으로 동일하지만, 어떤 데이터를 얼마나 하는지 pretrain이 중요하다\n",
        "# 이미 학습된 결과를 가져다 써야, 좋은 모델을 쓸 수 있다. 학습 체크 포인트. \n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
        "model = BertModel.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCpSWlYV7ja0",
        "colab_type": "text"
      },
      "source": [
        "##### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shaQM8GE9RIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'All the classes are provided.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAxXbZXW3ueJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']  # 이게 특징적이다!!!! bert의 6개 토큰이 들어가면 6개의 임베딩이 출력된다. \n",
        "# 하지만 어떤 토큰을 쓴건가 결정? 이 CLS라는 특별한 토크능ㄹ 사용하게 된다. 그래서 실제로는 분절화된 토큰 뿐 아니라 \n",
        "# 맨 앞의 CLS속성 이용하고 문장이 끝났다는 거르 의마흔 sep을 사용한다seperate의 약자\n",
        "# 문장이 두개가 입력된다면 무저건 맨 앞ㅇ는 cls하나만 들어가고 txt a, b구분하는 sep가 두개 들어가낟 \n",
        "# 문장이 하나일땐 scl,1ㄱ,sep1개지만 문장이 두개일땐 sep2는 2개 들어감\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wcGm1oJ4Df7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = [tokenizer.convert_tokens_to_ids(tokens)]\n",
        "print(ids) #토크나이저거 토큰을 id로 변환해서 컴퓨터에 숫자로 학습을 시켜야 한다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVa_OCH-31F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor = torch.tensor(ids)\n",
        "print(input_tensor.data)\n",
        "print(input_tensor.size()) #한문장 8개의 토큰 768개로 표현된다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns0LjLOd7Ry5",
        "colab_type": "text"
      },
      "source": [
        "##### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBGZFEn08vbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_tensor = model(input_tensor)[0] #구글이 제공하는 텐서. id를 텐서로 변환해주는 과정이 필요하다. 값은 동일한데 (안에)\n",
        "#  이거를 토치로 같은 텐서라 하더라도 파이토치와 텐서플러우의 텐서는 호환 안된다. \n",
        "# 적절한 것으로 변환하는게 필요하다\n",
        "print(hidden_tensor.size()) \n",
        "# 버트의 가장 작은 모델은 768다. 그래서 이거는 지켜줘야 한다.  큰 모델은 1024.. 이거는 뭔가 정해져 있는 값 같다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3yB6A3dCN6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logit = torch.nn.Linear(768, 3)(hidden_tensor) # 2를 3으로 바꾸기도 한다/차원임/\n",
        "print(logit.size()) # 768을 3으로 클래스가 3개일때 사람이냐 강아지냐 고양이냐. task있을때 이 세개중 어디가 가장 크게 나오느냐를 \n",
        "# 3으로 변환을 해주는데,,, 0.27ㅇ ㅣ가장 크면 첫번째 클래스를 선택하고, 그렂히 않은 경우 어디 선택하고~~~ \n",
        "print(logit.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5X-Id3DERC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = torch.nn.Softmax(dim=-1)(logit)  # 음수를 0~1사이 확률값으로 조정해주겠다. softmax이용. 합이 1이 됨. \n",
        "# 사람이 해석 가능한 값으로 치환한 것 \n",
        "# forward 방향으로 출력이 진행 됨. 예측된 결과가 얼마나 오차 큰지, 역으로 전파하며 back으로 가는데, 그거 트레이닝 과정 살펴봄\n",
        "# toturial 어떻게 자연어 변환해서, 최종 클래스를 내느냐를 살펴보았따. \n",
        "print(prediction.size())\n",
        "print(prediction.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKnl3-7yCnJ",
        "colab_type": "text"
      },
      "source": [
        "# **Emotion Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk8Fa9xrzndp",
        "colab_type": "text"
      },
      "source": [
        "##### **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzpFG2Azpx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "data = {'train': {'speaker': [], 'utterance': [], 'emotion': []},\n",
        "        'dev': {'speaker': [], 'utterance': [], 'emotion': []},\n",
        "        'test': {'speaker': [], 'utterance': [], 'emotion': []}}\n",
        "\n",
        "for dtype in ['train', 'dev', 'test']:\n",
        "  for dialog in json.loads(open('friends_' + dtype + '.json').read()):\n",
        "    for line in dialog:\n",
        "      data[dtype]['speaker'].append(line['speaker'])\n",
        "      data[dtype]['utterance'].append(line['utterance'])\n",
        "      data[dtype]['emotion'].append(line['emotion'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVhiZ12D6xXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e2i_dict = dict((emo, i) for i, emo in enumerate(set(data['train']['emotion'])))\n",
        "i2e_dict = {i: e for e, i in e2i_dict.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC1cs8JC3cYA",
        "colab_type": "text"
      },
      "source": [
        "##### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVU9HqHc3Tsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bert_tokenizer = BertTokenizer.from_pretrained(pretrained_weights) # 자연어를 어떻게 나눌지. 숫자 값으로 변환하고 \n",
        "    self.bert_model = BertModel.from_pretrained(pretrained_weights)   #버트 모델 768로 나올텐데, 클래스가 우리가 가진 이모션 갯수가 8개\n",
        "    self.linear = torch.nn.Linear(768, len(e2i_dict)) #버트 모델 중 하나를 예측하게 되는 것 \n",
        "\n",
        "  def forward(self, utterance):\n",
        "    tokens = self.bert_tokenizer.tokenize(utterance)\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]'] # (len)\n",
        "    ids = [tokenizer.convert_tokens_to_ids(tokens)] # (bat=1, len)\n",
        "    input_tensor = torch.tensor(ids).cuda()\n",
        "\n",
        "    hidden_tensor = self.bert_model(input_tensor)[0] # (bat, len, hid)\n",
        "    hidden_tensor = hidden_tensor[:, 0, :] # (bat, hid)\n",
        "    logit = self.linear(hidden_tensor)\n",
        "    return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rztPJu1DJI_G",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273RBAszJMGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate(true_list, pred_list):\n",
        "  precision = precision_score(true_list, pred_list, average=None)\n",
        "  recall = recall_score(true_list, pred_list, average=None)\n",
        "  micro_f1 = f1_score(true_list, pred_list, average='micro')\n",
        "  print('precision:\\t', ['%.4f' % v for v in precision])\n",
        "  print('recall:\\t\\t', ['%.4f' % v for v in recall])\n",
        "  print('micro_f1: %.6f' % micro_f1)\n",
        "  # 모델이 제대로 측정이 되었냐? 평가 메트릭을 어떻게 설정할 것인가? 얼마나 빠르게 학습 할건지, 몇번을 할건지.. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6gxWKi11OHV",
        "colab_type": "text"
      },
      "source": [
        "##### **Hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUY7pdLAyEDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = 'bert-base-uncased' # base->large로 써도 된다.\n",
        "# bert-base-uncased ->bert-large-uncased 로 바꿀 경우 위에서 1024로 바꿔야 한다. \n",
        "# bert-base-cased 로 써도 성능이 좋아질거다. \n",
        "# learning rate 바꿔도 좋아질거고, 에폭도 50, 100 이러게 하면 성능이 높아질거다. 이게 colab이라서 너무 오래 돌면.. 12시간 돌면 초기화 될꺼다. \n",
        "learning_rate = 1e-5  # 얼마나 학습을 빠르게 할꺼냐. 너무 빠르면, 성급한 일반화의 오류에,, 세세한 것에는 관심 못가질수도... \n",
        "# 적정수준 필요. 작게 잡는게 학습데이이터 많이 ㅣㄹ요하고 느리지만, 오랜시간 하습시 더 정확하다.\n",
        "n_epoch = 3  # 한 모델이 처음~끝, 한 에폭... 이 데이터를 여러번 읽음. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9RcP6vr1Whw",
        "colab_type": "text"
      },
      "source": [
        "##### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvetI0DNysM5",
        "colab_type": "code",
        "outputId": "eb6d5ab6-158a-46e6-e40d-2048fd5430d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import torch\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "model = Model()\n",
        "model.cuda() #mbdia GPU 생성하는데, 거기서 Mbida와 이게 호환 되도록 프로그램을 이용해 연결함. 이거를 cuda를 이용해 cpu쓰겠따는 뜻 \n",
        "criterion = torch.nn.CrossEntropyLoss() # LogSoftmax & NLLLoss  모델의 정확도. 맞췄냐 못맞췄냐..로스로 설정하겠다. 모델에 전달해서 모델 학습한다. \n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "for i_epoch in range(n_epoch):\n",
        "  print('i_epoch:', i_epoch)\n",
        "\n",
        "  model.train()\n",
        "  for i_batch in tqdm_notebook(range(len(data['train']['utterance']))):\n",
        "    logit = model(data['train']['utterance'][i_batch])\n",
        "    target = torch.tensor([e2i_dict[data['train']['emotion'][i_batch]]]).cuda()\n",
        "    loss = criterion(logit, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "\n",
        "  #  이거를 한 칸씩 들여 넣고, 몇번 했을때마다 모델 평가하는 형태로 바꾸면 조금 더 빠르게 할 수 이다. \n",
        "  model.eval()\n",
        "  pred_list, true_list = [], []\n",
        "  for i_batch in tqdm_notebook(range(len(data['dev']['utterance']))): # progress bar 이거는 노트북이라서 nb다. 그냥 tqdm notebook으로 하면 잘 나온다. \n",
        "    logit = model(data['dev']['utterance'][i_batch])\n",
        "    _, max_idx = torch.max(logit, dim=-1)\n",
        "    pred_list += max_idx.tolist()\n",
        "    true_list += [e2i_dict[data['dev']['emotion'][i_batch]]]\n",
        "  evaluate(pred_list, true_list) # print results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3d832c149534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Hh_8sUIFIn",
        "colab_type": "text"
      },
      "source": [
        "##### **Labeling**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po5XMyn5IP7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "labeled = []\n",
        "dialogs = json.loads(open('unlabeled.json').read())\n",
        "for dialog in tqdm_notebook(dialogs):\n",
        "  dialog_list = []\n",
        "  for line in dialog:\n",
        "    logit = model(line['utterance'])\n",
        "    _, max_idx = torch.max(logit, dim=-1)\n",
        "    pred_emotion = max_idx.tolist()[0]\n",
        "\n",
        "    line_dict = OrderedDict()\n",
        "    line_dict['speaker'] = line['speaker']\n",
        "    line_dict['utterance'] = line['utterance']\n",
        "    line_dict['emotion'] = i2e_dict[pred_emotion]\n",
        "    dialog_list.append(line_dict)\n",
        "  labeled.append(dialog_list)\n",
        "\n",
        "with open('labeled.json', 'w') as json_file:\n",
        "  json.dump(labeled, json_file, indent='\\t')\n",
        "\n",
        "  # 학습하고,, unlabled된거를 레이블링 하는 작업이다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3riAqyoPwhb",
        "colab_type": "text"
      },
      "source": [
        "##### **Proposal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWEWuZF31iGY",
        "colab_type": "text"
      },
      "source": [
        "- There is a class imbalance problem. (Use weighted cross-entropy etc.)\n",
        "\n",
        "- Our model takes a single sentence. (Make it grasp its context as well.)\n",
        "\n",
        "- Our model does not consider speaker information. (Make it consider the info.)\n",
        "\n",
        "- Batch size is set as 1. (Increase the batch size.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUOTTF6wrVOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습해서 labled를 내고, 파이썬 파일 수정한거 내고, 레포트는 각자 조사한거 제출하면 된다. \n",
        "\n",
        "# technical report :  버트 내부가 어덯게 하기때문에 이걸 사용했따던지.. 구체적으로 적어줄수록..좋을 것 같다. \n",
        "# 자료의 구성과 흐름... 성의... 기승전결.. 얼마나 잘 이뤄졌느냐. 이문제가 머가 왜 이걸 썻고, 그래서 어떻게 해결했꼬, 어떤 문제가 있다는 형태의 \n",
        "# 논문형태로 써주면 좋을 것 같습니다. \n",
        "\n",
        "# proposal 은 참고하라고 적어 주신거고. 알아서 개인적으로 의견 넣으면 된다. \n",
        "\n",
        "# 감정이 굉장히 biased 되어 있는데, 이거를 어떻게 균형잡히게 하면 될지, \n",
        "# crossendtrophyloss(weight....) 이렇게 넣으면 더 늘어난다. \n",
        "# 문장이 아니라 문맥을\n",
        "# 스피처.. 배치 사이즈를 늘려서 여러개를 종합해서,, \n",
        "# 로스를 개선학ㅆ다는 거라서 그거를 키우면 학습이 잘될꺼다. 개선점들을 바꿔봐라 \n",
        "# 화자가 누구인지를 넣으면 성능이 더 좋아질 수 있다. \n",
        "\n",
        "# jay alammar 의 딥러닝... 이거 참고할 것"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}